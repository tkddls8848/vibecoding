from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import time
import re
import os
import argparse
from datetime import datetime
import sys
from tqdm import tqdm
from parser import NaraParser, DataExporter
import concurrent.futures
import threading
import queue
import psutil
import gc
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException

# ì „ì—­ ë³€ìˆ˜
driver_pool = None

class OptimizedDriverPool:
    """ìµœì í™”ëœ WebDriver í’€ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, pool_size=10):
        self.pool_size = pool_size
        self.drivers = queue.Queue(maxsize=pool_size)
        self.lock = threading.Lock()
        self._initialize_pool()
    
    def _initialize_pool(self):
        """ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™”"""
        for _ in range(self.pool_size):
            driver = self._create_driver()
            self.drivers.put(driver)
    
    def _create_driver(self):
        """ìµœì í™”ëœ WebDriver ìƒì„±"""
        opts = Options()
        opts.add_argument("--headless")
        opts.add_argument("--no-sandbox")
        opts.add_argument("--disable-dev-shm-usage")
        opts.add_argument("--disable-gpu")
        opts.add_argument("--disable-extensions")
        opts.add_argument("--disable-plugins")
        opts.add_argument("--disable-background-timer-throttling")
        opts.add_argument("--disable-renderer-backgrounding")
        opts.add_argument("--disable-backgrounding-occluded-windows")
        opts.add_argument("--disable-features=TranslateUI")
        opts.add_argument("--disable-ipc-flooding-protection")
        opts.add_argument("--disable-images")
        opts.add_argument("--disable-css")
        opts.add_argument("--log-level=3")
        opts.add_argument("--no-default-browser-check")
        opts.add_argument("--no-first-run")
        opts.add_argument("--disable-default-apps")
        
        opts.add_experimental_option('excludeSwitches', ['enable-logging'])
        opts.add_experimental_option('useAutomationExtension', False)
        
        return webdriver.Chrome(options=opts)
    
    def get_driver(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë“œë¼ì´ë²„ ë°˜í™˜"""
        try:
            return self.drivers.get(timeout=5)
        except queue.Empty:
            return self._create_driver()
    
    def return_driver(self, driver):
        """ë“œë¼ì´ë²„ë¥¼ í’€ì— ë°˜í™˜"""
        try:
            driver.get("about:blank")
            driver.delete_all_cookies()
            if self.drivers.qsize() < self.pool_size:
                self.drivers.put(driver)
            else:
                driver.quit()
        except:
            try:
                driver.quit()
            except:
                pass
    
    def close_all(self):
        """ëª¨ë“  ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        while not self.drivers.empty():
            try:
                driver = self.drivers.get_nowait()
                driver.quit()
            except:
                pass

class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    @staticmethod
    def get_memory_usage():
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # MB ë‹¨ìœ„
    
    @staticmethod
    def check_memory_threshold(threshold_mb=2000):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸"""
        return MemoryManager.get_memory_usage() > threshold_mb
    
    @staticmethod
    def cleanup():
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()

def get_api_id(url):
    """URLì—ì„œ API ID ì¶”ì¶œ"""
    m = re.search(r'/data/(\d+)/openapi', url)
    return m.group(1) if m else f"api_{url.replace('https://', '').replace('/', '_')}"

def crawl_url(url, output_dir, formats, driver_pool):
    """ë‹¨ì¼ URL í¬ë¡¤ë§ - ê°œì„ ëœ ìˆœì„œ"""
    os.makedirs(output_dir, exist_ok=True)
    api_id = get_api_id(url)
    
    driver = driver_pool.get_driver()
    
    crawling_result = {
        'success': False,
        'data': None,
        'saved_files': [],
        'errors': [],
        'api_id': api_id,
        'url': url
    }
    
    try:
        driver.get(url)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(1)
        
        parser = NaraParser(driver)
        
        # ê²°ê³¼ ë°ì´í„° ì´ˆê¸°í™”
        result = {
            'api_id': api_id,
            'crawled_url': url,
            'crawled_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Step 1: ìš°ì„ ì ìœ¼ë¡œ í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ
        table_info = parser.extract_table_info()
        result['info'] = table_info
        
        # API ìœ í˜• í™•ì¸
        api_type_field = table_info.get('API ìœ í˜•', '').upper()
        
        # Step 2: API ìœ í˜•ì´ LINKì¸ ê²½ìš° í…Œì´ë¸” í¬ë¡¤ë§ë§Œ í•˜ê³  ì¢…ë£Œ
        if 'LINK' in api_type_field:
            result['api_type'] = 'link'
            result['skip_reason'] = 'LINK íƒ€ì… APIëŠ” í…Œì´ë¸” ì •ë³´ë§Œ ìˆ˜ì§‘'
            
            crawling_result['data'] = result
            crawling_result['success'] = True
            
            # ë°ì´í„° ì €ì¥
            saved_files, save_errors = DataExporter.save_crawling_result(result, output_dir, api_id, formats)
            
            crawling_result['saved_files'] = saved_files
            crawling_result['errors'] = save_errors
            
            return crawling_result['success']
        
        # Step 3: REST APIì¸ ê²½ìš° Swagger JSON ì¶”ì¶œ ì‹œë„
        swagger_json = parser.extract_swagger_json()
        
        if swagger_json and isinstance(swagger_json, dict) and swagger_json:
            # Swagger JSONì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            # API ì •ë³´ ì¶”ì¶œ
            api_info = parser.extract_api_info(swagger_json)
            
            # Base URL ì¶”ì¶œ
            base_url = parser.extract_base_url(swagger_json)
            api_info['base_url'] = base_url
            
            # Schemes ì¶”ì¶œ
            api_info['schemes'] = swagger_json.get('schemes', ['https'])
            
            # ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
            endpoints = parser.extract_endpoints(swagger_json)
            
            # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
            result.update({
                'api_info': api_info,
                'endpoints': endpoints,
                'swagger_json': swagger_json,
                'api_type': 'swagger'
            })
            
        else:
            # Step 4: Swagger JSONì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ API ì •ë³´ ì¶”ì¶œ
            general_api_info = parser.extract_general_api_info()
            
            if general_api_info and (general_api_info.get('detail_info') or 
                                   general_api_info.get('request_parameters') or 
                                   general_api_info.get('response_elements')):
                # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
                result.update({
                    'general_api_info': general_api_info,
                    'api_type': 'general'
                })
            else:
                # ì •ë³´ë¶€ì¡± URLì„ ë³„ë„ íŒŒì¼ì— ì €ì¥
                failed_urls_file = os.path.join(output_dir, "failed_urls.txt")
                os.makedirs(output_dir, exist_ok=True)
                with open(failed_urls_file, 'a', encoding='utf-8') as f:
                    f.write(f"{url}\n")
                
                crawling_result['errors'].append("API ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ failed_urls.txtì— ê¸°ë¡")
                return False
        
        crawling_result['data'] = result
        crawling_result['success'] = True
        
        # ë°ì´í„° ì €ì¥
        saved_files, save_errors = DataExporter.save_crawling_result(result, output_dir, api_id, formats)
        
        crawling_result['saved_files'] = saved_files
        crawling_result['errors'] = save_errors
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        MemoryManager.cleanup()
        
        return crawling_result['success']
    
    except Exception as e:
        error_msg = f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}"
        print(f"âŒ {error_msg}")
        crawling_result['errors'].append(error_msg)
        return False
    finally:
        driver_pool.return_driver(driver)

def generate_urls(start_num, end_num):
    """ì‹œì‘ë²ˆí˜¸ì™€ ëë²ˆí˜¸ ì‚¬ì´ì˜ ëª¨ë“  URL ìƒì„±"""
    base_url = "https://www.data.go.kr/data/{}/openapi.do"
    return [base_url.format(num) for num in range(start_num, end_num + 1)]

def batch_crawl(urls, output_dir="data", formats=['json', 'xml', 'md', 'csv'], max_workers=40):
    """ë²”ìœ„ ë‚´ì˜ ëª¨ë“  API ë¬¸ì„œ í¬ë¡¤ë§"""
    total_urls = len(urls)
    
    print(f"\nğŸš€ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘")
    print(f"   ğŸ“‹ ì´ {total_urls}ê°œ URL")
    print(f"   ğŸ‘¥ ë™ì‹œ ì‘ì—…ì: {max_workers}ê°œ")
    print(f"   ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"   ğŸ’¾ ì €ì¥ í˜•ì‹: {', '.join(formats)}")
    
    # ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™”
    driver_pool = OptimizedDriverPool(pool_size=max_workers)
    
    # ê²°ê³¼ ì €ì¥ìš© ë³€ìˆ˜
    results = {
        'total': total_urls,
        'success': 0,
        'failed': 0,
        'link_type': 0,
        'swagger_type': 0,
        'general_type': 0,
        'insufficient_info': 0,  # ì •ë³´ë¶€ì¡± ì¹´ìš´íŠ¸ ì¶”ê°€
        'failed_urls': [],
        'start_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {
                executor.submit(crawl_url, url, output_dir, formats, driver_pool): url 
                for url in urls
            }
            
            with tqdm(total=total_urls, desc="í¬ë¡¤ë§ ì§„í–‰") as pbar:
                for future in concurrent.futures.as_completed(future_to_url):
                    url = future_to_url[future]
                    
                    if MemoryManager.check_memory_threshold():
                        print("\nâš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ì •ë¦¬ ì¤‘...")
                        MemoryManager.cleanup()
                    
                    try:
                        result = future.result()
                        if result:
                            results['success'] += 1
                        else:
                            results['failed'] += 1
                    except Exception as e:
                        results['failed'] += 1
                        results['failed_urls'].append(url)
                        print(f"\nâš ï¸ ì˜ˆì™¸ ë°œìƒ: {url} - {str(e)}")
                    
                    pbar.update(1)
                    
                    if pbar.n % 10 == 0:
                        MemoryManager.cleanup()
    
    finally:
        driver_pool.close_all()
    
    # ê²°ê³¼ ìš”ì•½
    results['end_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    results['success_rate'] = f"{(results['success'] / total_urls * 100):.1f}%" if total_urls > 0 else "0%"
    
    # ê²°ê³¼ ì €ì¥
    summary_file = os.path.join(output_dir, "crawling_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # ì‹¤íŒ¨í•œ URL ëª©ë¡ ì €ì¥ (ì˜ˆì™¸ ë°œìƒí•œ URLë§Œ)
    if results['failed_urls']:
        exception_urls_file = os.path.join(output_dir, "exception_urls.txt")
        with open(exception_urls_file, 'w', encoding='utf-8') as f:
            for url in results['failed_urls']:
                f.write(f"{url}\n")
    
    # failed_urls.txt íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ì •ë³´ë¶€ì¡± URLë“¤)
    failed_urls_file = os.path.join(output_dir, "failed_urls.txt")
    insufficient_info_count = 0
    if os.path.exists(failed_urls_file):
        with open(failed_urls_file, 'r', encoding='utf-8') as f:
            insufficient_info_count = len(f.readlines())
        results['insufficient_info'] = insufficient_info_count
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 50)
    print("ğŸ ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("=" * 50)
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼:")
    print(f"   ğŸ“‹ ì´ ì²˜ë¦¬: {results['total']}ê°œ URL")
    print(f"   âœ… ì„±ê³µ: {results['success']}ê°œ ({results['success_rate']})")
    print(f"   âŒ ì‹¤íŒ¨: {results['failed']}ê°œ")
    if insufficient_info_count > 0:
        print(f"   ğŸ“ ì •ë³´ë¶€ì¡±: {insufficient_info_count}ê°œ (failed_urls.txtì— ê¸°ë¡)")
    print(f"   ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {output_dir}")
    print(f"   ğŸ“‹ ìš”ì•½ íŒŒì¼: crawling_summary.json")
    if results['failed'] > 0:
        print(f"   ğŸ“„ ì˜ˆì™¸ ëª©ë¡: exception_urls.txt")
    if insufficient_info_count > 0:
        print(f"   ğŸ“„ ì •ë³´ë¶€ì¡± ëª©ë¡: failed_urls.txt")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬')
    parser.add_argument('-s', '--start', type=int, required=True, help='ì‹œì‘ ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-e', '--end', type=int, required=True, help='ë ë¬¸ì„œ ë²ˆí˜¸')
    parser.add_argument('-o', '--output-dir', default='output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)')
    parser.add_argument('--formats', nargs='+', default=['json', 'xml', 'md', 'csv'],
                      choices=['json', 'xml', 'md', 'csv'], help='ì €ì¥í•  íŒŒì¼ í˜•ì‹')
    parser.add_argument('-w', '--workers', type=int, default=20, help='ë™ì‹œ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 20)')
    parser.add_argument('--no-headless', action='store_true', help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ë¹„í™œì„±í™”')
    parser.add_argument('--timeout', type=int, default=5, help='í˜ì´ì§€ ë¡œë“œ íƒ€ì„ì•„ì›ƒ (ì´ˆ)')
    
    args = parser.parse_args()
    
    # ì…ë ¥ê°’ ê²€ì¦
    if args.start > args.end:
        print("âŒ ì‹œì‘ ë²ˆí˜¸ê°€ ë ë²ˆí˜¸ë³´ë‹¤ í´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    if args.workers < 10 or args.workers > 40:
        print("âš ï¸ ë™ì‹œ ì‘ì—…ì ìˆ˜ëŠ” 10-40 ì‚¬ì´ë¡œ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        args.workers = 20
    
    # ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
    batch_crawl(
        generate_urls(args.start, args.end),
        args.output_dir,
        args.formats,
        args.workers
    )

if __name__ == '__main__':
    main()