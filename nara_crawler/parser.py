import json
import re
from selenium.webdriver.common.by import By
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
import os
from datetime import datetime
import requests
import csv
from functools import lru_cache
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor

class NaraParser:
    """ë‚˜ë¼ì¥í„° API íŒŒì„œ í´ë˜ìŠ¤"""
    
    def __init__(self, driver):
        self.driver = driver
        self._thread_pool = ThreadPoolExecutor(max_workers=4)
    
    def extract_table_info(self):
        """í…Œì´ë¸” ì •ë³´ ì¶”ì¶œ - ìµœìš°ì„  ì‹¤í–‰ (UDDI ê°’ ì¶”ì¶œ í¬í•¨)"""
        try:
            table_info = {}
            
            # ëª¨ë“  í…Œì´ë¸” ì°¾ê¸°
            tables = self.driver.find_elements(By.CSS_SELECTOR, "table.dataset-table")
            
            for table in tables:
                # í…Œì´ë¸” ë‚´ìš© ì¶”ì¶œ
                rows = table.find_elements(By.TAG_NAME, "tr")
                
                for row in rows:
                    try:
                        # thì™€ td íƒœê·¸ ì°¾ê¸°
                        th = row.find_element(By.TAG_NAME, "th")
                        td = row.find_element(By.TAG_NAME, "td")
                        
                        key = th.text.strip()
                        value = td.text.strip()
                        
                        # ì „í™”ë²ˆí˜¸ì˜ ê²½ìš° JavaScriptë¡œ ì²˜ë¦¬ëœ ê°’ì„ ê°€ì ¸ì˜¤ê¸°
                        if "ì „í™”ë²ˆí˜¸" in key:
                            try:
                                tel_no_div = td.find_element(By.ID, "telNoDiv")
                                value = tel_no_div.text.strip()
                            except:
                                pass
                        
                        # ë§í¬ê°€ ìˆëŠ” ê²½ìš° ë§í¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        if not value:
                            try:
                                link = td.find_element(By.TAG_NAME, "a")
                                value = link.text.strip()
                            except:
                                pass
                        
                        if key and value:
                            table_info[key] = value
                    except Exception as e:
                        continue
            
            # API ìœ í˜• í™•ì¸ ë° ë¡œê¹…
            api_type = table_info.get('API ìœ í˜•', '')
            if api_type:
                if 'LINK' in api_type.upper():
                    pass
                else:
                    # LINK íƒ€ì…ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ UDDI ê°’ ì¶”ì¶œ
                    uddi_value = self._extract_uddi_value()
                    if uddi_value:
                        table_info['uddi'] = uddi_value
                        
                        # UDDI ê°’ì„ íŒŒì¼ì— ì €ì¥
                        self._save_uddi_to_file(uddi_value, self.driver.current_url)
            
            return table_info
            
        except Exception as e:
            return {}

    def _extract_uddi_value(self):
        """UDDI ê°’ ì¶”ì¶œ"""
        try:
            # hidden input ìš”ì†Œì—ì„œ UDDI ê°’ ì°¾ê¸°
            uddi_input = self.driver.find_element(By.ID, "publicDataDetailPk")
            uddi_value = uddi_input.get_attribute("value")
            
            if uddi_value and uddi_value.strip():
                return uddi_value.strip()
            else:
                return None
                
        except Exception as e:
            # ëŒ€ì•ˆ: ëª¨ë“  hidden inputì—ì„œ ì°¾ê¸°
            try:
                hidden_inputs = self.driver.find_elements(By.CSS_SELECTOR, "input[type='hidden']")
                
                for input_elem in hidden_inputs:
                    input_id = input_elem.get_attribute("id")
                    input_value = input_elem.get_attribute("value")
                    
                    # publicDataDetailPk ë˜ëŠ” ìœ ì‚¬í•œ ID íŒ¨í„´ í™•ì¸
                    if input_id and ("publicDataDetailPk" in input_id.lower() or 
                                   "uddi" in input_id.lower() or 
                                   "detailpk" in input_id.lower()):
                        if input_value and input_value.strip():
                            return input_value.strip()
                
                return None
                
            except Exception as e2:
                return None

    def _save_uddi_to_file(self, uddi_value, current_url):
        """UDDI ê°’ì„ íŒŒì¼ì— ì €ì¥"""
        try:
            # uddi.txt íŒŒì¼ ê²½ë¡œ ì„¤ì •
            uddi_file_path = "uddi.txt"
            
            # í˜„ì¬ ì‹œê°„ ì •ë³´
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # íŒŒì¼ì— ì¶”ê°€ ëª¨ë“œë¡œ ì €ì¥
            with open(uddi_file_path, 'a', encoding='utf-8') as f:
                f.write(f"{uddi_value}\t{current_url}\t{current_time}\n")
            
        except Exception as e:
            pass

    @lru_cache(maxsize=100)
    def extract_swagger_json(self):
        """Swagger JSON ì¶”ì¶œ - ê°œì„ ëœ ë¡œì§"""
        try:
            # 1. JavaScript ë³€ìˆ˜ì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
            swagger_json = self.driver.execute_script("""
                try {
                    if (typeof swaggerJson !== 'undefined' && swaggerJson !== null) {
                        if (typeof swaggerJson === 'string') {
                            if (swaggerJson.trim() === '') {
                                return null;
                            }
                            return JSON.parse(swaggerJson);
                        } else if (typeof swaggerJson === 'object') {
                            return swaggerJson;
                        }
                    }
                    return null;
                } catch (e) {
                    return null;
                }
            """)
            
            if swagger_json and isinstance(swagger_json, dict) and swagger_json:
                return swagger_json
            
            # 2. script íƒœê·¸ì—ì„œ swaggerJson ë³€ìˆ˜ ì¶”ì¶œ ì‹œë„
            scripts = self.driver.find_elements(By.TAG_NAME, "script")
            for script in scripts:
                script_content = script.get_attribute("innerHTML")
                if script_content and 'swaggerJson' in script_content:
                    # ë¹ˆ swaggerJson íŒ¨í„´ ë¨¼ì € í™•ì¸
                    empty_patterns = [
                        r'var\s+swaggerJson\s*=\s*[\'\"]\s*[\'\"]\s*[;,]',
                        r'swaggerJson\s*=\s*[\'\"]\s*[\'\"]\s*[;,]',
                        r'var\s+swaggerJson\s*=\s*`\s*`\s*[;,]',
                        r'swaggerJson\s*=\s*`\s*`\s*[;,]'
                    ]
                    
                    for pattern in empty_patterns:
                        if re.search(pattern, script_content):
                            return None
                    
                    # swaggerJson ê°’ ì¶”ì¶œ íŒ¨í„´ë“¤
                    json_patterns = [
                        r'var\s+swaggerJson\s*=\s*(\{.*?\})\s*[;,]',  # var swaggerJson = {...};
                        r'swaggerJson\s*=\s*(\{.*?\})\s*[;,]',        # swaggerJson = {...};
                        r'swaggerJson\s*:\s*(\{.*?\})',               # swaggerJson: {...}
                        r'var\s+swaggerJson\s*=\s*`(\{.*?\})`',       # var swaggerJson = `{...}`;
                        r'swaggerJson\s*=\s*`(\{.*?\})`'              # swaggerJson = `{...}`;
                    ]
                    
                    for pattern in json_patterns:
                        json_match = re.search(pattern, script_content, re.DOTALL)
                        if json_match:
                            try:
                                json_str = json_match.group(1)
                                # JSON ë¬¸ìì—´ ì •ë¦¬
                                json_str = json_str.replace('\n', '').replace('\r', '')
                                parsed_json = json.loads(json_str)
                                if parsed_json:  # ë¹ˆ ê°ì²´ê°€ ì•„ë‹Œ ê²½ìš°
                                    return parsed_json
                            except Exception as e:
                                continue
            
            # 3. window.swaggerUi ë³€ìˆ˜ì—ì„œ ì¶”ì¶œ ì‹œë„
            swagger_json = self.driver.execute_script("""
                try {
                    if (window.swaggerUi && window.swaggerUi.spec) {
                        return window.swaggerUi.spec;
                    }
                    return null;
                } catch (e) {
                    return null;
                }
            """)
            
            if swagger_json and isinstance(swagger_json, dict) and swagger_json:
                return swagger_json
            
            # 4. Swagger UI ì´ˆê¸°í™” ì½”ë“œì—ì„œ URL ì¶”ì¶œ ì‹œë„
            for script in scripts:
                script_content = script.get_attribute("innerHTML")
                if script_content and 'SwaggerUIBundle' in script_content:
                    # URL íŒ¨í„´ ì°¾ê¸°
                    url_match = re.search(r'url\s*:\s*[\'"]([^\'"]+)[\'"]', script_content)
                    if url_match:
                        swagger_url = url_match.group(1)
                        if swagger_url.startswith('/'):
                            current_url = self.driver.current_url
                            base_url = '/'.join(current_url.split('/')[:3])
                            swagger_url = base_url + swagger_url
                        
                        try:
                            response = requests.get(swagger_url, timeout=10)
                            if response.status_code == 200:
                                swagger_data = response.json()
                                if swagger_data:
                                    return swagger_data
                        except Exception as e:
                            pass
                    
                    # ì¸ë¼ì¸ spec ê°ì²´ ì°¾ê¸°
                    spec_match = re.search(r'spec\s*:\s*(\{.*?\})\s*[,}]', script_content, re.DOTALL)
                    if spec_match:
                        try:
                            spec_str = spec_match.group(1)
                            spec_json = json.loads(spec_str)
                            if spec_json:
                                return spec_json
                        except Exception as e:
                            pass
            
            return None
            
        except Exception as e:
            return None

    def extract_general_api_info(self):
        """ì¼ë°˜ API ì •ë³´ ì¶”ì¶œ (Swaggerê°€ ì—†ëŠ” ê²½ìš°)"""
        try:
            general_info = {}
            
            # 1. ìƒì„¸ê¸°ëŠ¥ ì •ë³´ ì¶”ì¶œ
            detail_info = self._extract_detail_info()
            if detail_info:
                general_info['detail_info'] = detail_info
            
            # 2. ìš”ì²­ë³€ìˆ˜(Request Parameter) ì¶”ì¶œ
            request_params = self._extract_request_parameters()
            if request_params:
                general_info['request_parameters'] = request_params
            
            # 3. ì¶œë ¥ê²°ê³¼(Response Element) ì¶”ì¶œ
            response_elements = self._extract_response_elements()
            if response_elements:
                general_info['response_elements'] = response_elements
            
            return general_info
            
        except Exception as e:
            return {}
    
    def _extract_detail_info(self):
        """ìƒì„¸ê¸°ëŠ¥ ì •ë³´ ì¶”ì¶œ"""
        try:
            detail_info = {}
            
            # open-api-detail-result div ì°¾ê¸°
            detail_div = self.driver.find_element(By.ID, "open-api-detail-result")
            
            # h4.tit ë‚´ìš© ì¶”ì¶œ (API ì„¤ëª…)
            try:
                title_elem = detail_div.find_element(By.CSS_SELECTOR, "h4.tit")
                detail_info['description'] = title_elem.text.strip()
            except:
                detail_info['description'] = ""
            
            # box-gray í•˜ìœ„ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            try:
                box_gray = detail_div.find_element(By.CLASS_NAME, "box-gray")
                list_items = box_gray.find_elements(By.CSS_SELECTOR, "ul.dot-list li")
                
                for item in list_items:
                    item_text = item.text.strip()
                    
                    # í™œìš©ìŠ¹ì¸ ì ˆì°¨
                    if "í™œìš©ìŠ¹ì¸ ì ˆì°¨" in item_text:
                        # ê°œë°œë‹¨ê³„ì™€ ìš´ì˜ë‹¨ê³„ ì •ë³´ ì¶”ì¶œ
                        dev_match = re.search(r'ê°œë°œë‹¨ê³„\s*:\s*([^/]+)', item_text)
                        op_match = re.search(r'ìš´ì˜ë‹¨ê³„\s*:\s*(.+)', item_text)
                        
                        approval_process = {}
                        if dev_match:
                            approval_process['development'] = dev_match.group(1).strip()
                        if op_match:
                            approval_process['operation'] = op_match.group(1).strip()
                        
                        detail_info['approval_process'] = approval_process
                    
                    # ì‹ ì²­ê°€ëŠ¥ íŠ¸ë˜í”½
                    elif "ì‹ ì²­ê°€ëŠ¥ íŠ¸ë˜í”½" in item_text:
                        # ê°œë°œê³„ì •ê³¼ ìš´ì˜ê³„ì • ì •ë³´ ì¶”ì¶œ
                        dev_traffic_match = re.search(r'ê°œë°œê³„ì •\s*:\s*([^/]+)', item_text)
                        op_traffic_match = re.search(r'ìš´ì˜ê³„ì •\s*:\s*(.+)', item_text)
                        
                        traffic_info = {}
                        if dev_traffic_match:
                            traffic_info['development'] = dev_traffic_match.group(1).strip()
                        if op_traffic_match:
                            traffic_info['operation'] = op_traffic_match.group(1).strip()
                        
                        detail_info['traffic_limit'] = traffic_info
                    
                    # ìš”ì²­ì£¼ì†Œ
                    elif "ìš”ì²­ì£¼ì†Œ" in item_text:
                        url_match = re.search(r'ìš”ì²­ì£¼ì†Œ\s*(.+)', item_text)
                        if url_match:
                            detail_info['request_url'] = url_match.group(1).strip()
                    
                    # ì„œë¹„ìŠ¤URL
                    elif "ì„œë¹„ìŠ¤URL" in item_text:
                        service_url_match = re.search(r'ì„œë¹„ìŠ¤URL\s*(.+)', item_text)
                        if service_url_match:
                            detail_info['service_url'] = service_url_match.group(1).strip()
            except Exception as e:
                pass
            
            return detail_info
            
        except Exception as e:
            return {}
    
    def _extract_request_parameters(self):
        """ìš”ì²­ë³€ìˆ˜(Request Parameter) í…Œì´ë¸” ì¶”ì¶œ"""
        try:
            parameters = []
            
            # ìš”ì²­ë³€ìˆ˜ ì„¹ì…˜ ì°¾ê¸°
            headers = self.driver.find_elements(By.CSS_SELECTOR, "h4.tit")
            request_header = None
            
            for header in headers:
                if "ìš”ì²­ë³€ìˆ˜" in header.text and "Request Parameter" in header.text:
                    request_header = header
                    break
            
            if not request_header:
                return parameters
            
            # ìš”ì²­ë³€ìˆ˜ í…Œì´ë¸” ì°¾ê¸° (í—¤ë” ë‹¤ìŒ div.col-table)
            table_div = request_header.find_element(By.XPATH, "following-sibling::div[contains(@class, 'col-table')]")
            table = table_div.find_element(By.TAG_NAME, "table")
            
            # í…Œì´ë¸” í–‰ ì¶”ì¶œ (í—¤ë” ì œì™¸)
            tbody = table.find_element(By.TAG_NAME, "tbody")
            rows = tbody.find_elements(By.TAG_NAME, "tr")
            
            for row in rows:
                try:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 6:  # ìµœì†Œ 6ê°œ ì—´ì´ ìˆì–´ì•¼ í•¨
                        parameter = {
                            'name_kor': cells[0].text.strip(),          # í•­ëª©ëª…(êµ­ë¬¸)
                            'name_eng': cells[1].text.strip(),          # í•­ëª©ëª…(ì˜ë¬¸)
                            'size': cells[2].text.strip(),              # í•­ëª©í¬ê¸°
                            'required': cells[3].text.strip(),          # í•­ëª©êµ¬ë¶„ (í•„/ì˜µ)
                            'sample_data': cells[4].text.strip(),       # ìƒ˜í”Œë°ì´í„°
                            'description': cells[5].text.strip()        # í•­ëª©ì„¤ëª…
                        }
                        
                        # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        if parameter['name_eng'] or parameter['name_kor']:
                            parameters.append(parameter)
                            
                except Exception as e:
                    continue
            
            return parameters
            
        except Exception as e:
            return []
    
    def _extract_response_elements(self):
        """ì¶œë ¥ê²°ê³¼(Response Element) í…Œì´ë¸” ì¶”ì¶œ"""
        try:
            elements = []
            
            # ì¶œë ¥ê²°ê³¼ ì„¹ì…˜ ì°¾ê¸°
            headers = self.driver.find_elements(By.CSS_SELECTOR, "h4.tit")
            response_header = None
            
            for header in headers:
                if "ì¶œë ¥ê²°ê³¼" in header.text and "Response Element" in header.text:
                    response_header = header
                    break
            
            if not response_header:
                return elements
            
            # ì¶œë ¥ê²°ê³¼ í…Œì´ë¸” ì°¾ê¸° (í—¤ë” ë‹¤ìŒ div.col-table)
            table_div = response_header.find_element(By.XPATH, "following-sibling::div[contains(@class, 'col-table')]")
            table = table_div.find_element(By.TAG_NAME, "table")
            
            # í…Œì´ë¸” í–‰ ì¶”ì¶œ (í—¤ë” ì œì™¸)
            tbody = table.find_element(By.TAG_NAME, "tbody")
            rows = tbody.find_elements(By.TAG_NAME, "tr")
            
            for row in rows:
                try:
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 6:  # ìµœì†Œ 6ê°œ ì—´ì´ ìˆì–´ì•¼ í•¨
                        element = {
                            'name_kor': cells[0].text.strip(),          # í•­ëª©ëª…(êµ­ë¬¸)
                            'name_eng': cells[1].text.strip(),          # í•­ëª©ëª…(ì˜ë¬¸)
                            'size': cells[2].text.strip(),              # í•­ëª©í¬ê¸°
                            'required': cells[3].text.strip(),          # í•­ëª©êµ¬ë¶„ (í•„/ì˜µ)
                            'sample_data': cells[4].text.strip(),       # ìƒ˜í”Œë°ì´í„°
                            'description': cells[5].text.strip()        # í•­ëª©ì„¤ëª…
                        }
                        
                        # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        if element['name_eng'] or element['name_kor']:
                            elements.append(element)
                            
                except Exception as e:
                    continue
            
            return elements
            
        except Exception as e:
            return []
    
    def extract_api_info(self, swagger_json):
        """API ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        api_info = {}
        
        if not swagger_json:
            return api_info
            
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        info = swagger_json.get('info', {})
        api_info['title'] = info.get('title', '')
        api_info['description'] = info.get('description', '')
        api_info['version'] = info.get('version', '')
        
        # í™•ì¥ ì •ë³´ ì¶”ì¶œ
        if 'x-' in info:
            for key, value in info.items():
                if key.startswith('x-'):
                    api_info[key.replace('x-', '')] = value
        
        return api_info
    
    def extract_base_url(self, swagger_json):
        """Base URL ì¶”ì¶œ"""
        if not swagger_json:
            return ""
            
        schemes = swagger_json.get('schemes', ['https'])
        host = swagger_json.get('host', '')
        base_path = swagger_json.get('basePath', '')
        
        if host:
            scheme = schemes[0] if schemes else 'https'
            return f"{scheme}://{host}{base_path}"
        return ""
    
    def extract_endpoints(self, swagger_json):
        """ì—”ë“œí¬ì¸íŠ¸ ì •ë³´ ì¶”ì¶œ"""
        endpoints = []
        
        if not swagger_json:
            return endpoints
            
        paths = swagger_json.get('paths', {})
        
        for path, methods in paths.items():
            for method, data in methods.items():
                if method in ['get', 'post', 'put', 'delete', 'patch']:
                    endpoint = {
                        'method': method.upper(),
                        'path': path,
                        'description': data.get('summary', '') or data.get('description', ''),
                        'parameters': self._extract_parameters(data.get('parameters', [])),
                        'responses': self._extract_responses(data.get('responses', {})),
                        'tags': data.get('tags', []),
                        'section': data.get('tags', ['Default'])[0] if data.get('tags') else 'Default'
                    }
                    endpoints.append(endpoint)
        
        return endpoints
    
    def _extract_parameters(self, params_list):
        """íŒŒë¼ë¯¸í„° ì •ë³´ ì¶”ì¶œ"""
        parameters = []
        
        for param in params_list:
            parameters.append({
                'name': param.get('name', ''),
                'description': param.get('description', ''),
                'required': param.get('required', False),
                'type': param.get('type', '') or (param.get('schema', {}).get('type', '') if 'schema' in param else '')
            })
        
        return parameters
    
    def _extract_responses(self, responses_dict):
        """ì‘ë‹µ ì •ë³´ ì¶”ì¶œ"""
        responses = []
        
        for status_code, data in responses_dict.items():
            responses.append({
                'status_code': status_code,
                'description': data.get('description', '')
            })
        
        return responses


class DataExporter:
    """ë°ì´í„° ë‚´ë³´ë‚´ê¸° í´ë˜ìŠ¤"""
    
    @staticmethod
    def save_as_json(data, file_path):
        """JSON í˜•íƒœë¡œ ì €ì¥"""
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            return True, None
        except Exception as e:
            return False, f"JSON ì €ì¥ ì‹¤íŒ¨: {str(e)}"
    
    @staticmethod
    def dict_to_xml(data, root_name="api_documentation"):
        """ë”•ì…”ë„ˆë¦¬ë¥¼ XMLë¡œ ë³€í™˜"""
        try:
            def _dict_to_xml_element(d, parent, name=None):
                if name is None:
                    element = parent
                else:
                    # XML íƒœê·¸ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ìœ íš¨ì„± ê²€ì‚¬
                    clean_name = re.sub(r'[^a-zA-Z0-9_-]', '_', str(name))
                    # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íƒœê·¸ëª… ì²˜ë¦¬
                    if clean_name and clean_name[0].isdigit():
                        clean_name = f"item_{clean_name}"
                    # ë¹ˆ íƒœê·¸ëª… ì²˜ë¦¬
                    if not clean_name:
                        clean_name = "unnamed_item"
                    
                    element = SubElement(parent, clean_name)
                
                if isinstance(d, dict):
                    for key, value in d.items():
                        _dict_to_xml_element(value, element, key)
                elif isinstance(d, list):
                    for i, item in enumerate(d):
                        if isinstance(item, dict):
                            _dict_to_xml_element(item, element, f"item_{i}")
                        else:
                            item_elem = SubElement(element, f"item_{i}")
                            item_elem.text = str(item) if item is not None else ""
                else:
                    element.text = str(d) if d is not None else ""
            
            root = Element(root_name)
            _dict_to_xml_element(data, root)
            return root, None
        except Exception as e:
            return None, f"XML ë³€í™˜ ì‹¤íŒ¨: {str(e)}"
    
    @staticmethod
    def save_as_xml(data, file_path):
        """XML í˜•íƒœë¡œ ì €ì¥"""
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
            
            # ë”•ì…”ë„ˆë¦¬ë¥¼ XMLë¡œ ë³€í™˜
            root, error = DataExporter.dict_to_xml(data)
            if error:
                return False, error
            
            # ì˜ˆì˜ê²Œ í¬ë§·íŒ…
            rough_string = tostring(root, encoding='utf-8')
            reparsed = minidom.parseString(rough_string)
            pretty_xml = reparsed.toprettyxml(indent='  ', encoding='utf-8')
            
            with open(file_path, 'wb') as f:
                f.write(pretty_xml)
            
            return True, None
        except Exception as e:
            return False, f"XML ì €ì¥ ì‹¤íŒ¨: {str(e)}"
    
    @staticmethod
    def save_as_markdown(data, file_path):
        """Markdown í˜•íƒœë¡œ ì €ì¥"""
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
            
            md_content = DataExporter.dict_to_markdown(data)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            return True, None
        except Exception as e:
            return False, f"Markdown ì €ì¥ ì‹¤íŒ¨: {str(e)}"
    
    @staticmethod
    def dict_to_markdown(data):
        """ë”•ì…”ë„ˆë¦¬ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            md_lines = []
            api_type = data.get('api_type', 'unknown')
            
            # API íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬ ë¶„ê¸°
            if api_type == 'swagger':
                return DataExporter._swagger_to_markdown(data, md_lines)
            elif api_type == 'general':
                return DataExporter._general_api_to_markdown(data, md_lines)
            elif api_type == 'link':
                return DataExporter._link_to_markdown(data, md_lines)
            else:
                return "# API ë¬¸ì„œ\n\nì•Œ ìˆ˜ ì—†ëŠ” API íƒ€ì…ì…ë‹ˆë‹¤."
                
        except Exception as e:
            print(f"âš ï¸ Markdown ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"# Markdown ë³€í™˜ ì˜¤ë¥˜\n\në³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    @staticmethod
    def _link_to_markdown(data, md_lines):
        """LINK íƒ€ì… APIë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        md_lines.append("# LINK íƒ€ì… API")
        md_lines.append("")
        
        # í¬ë¡¤ë§ ì •ë³´
        if data.get('crawled_time'):
            md_lines.append(f"**í¬ë¡¤ë§ ì‹œê°„:** {data['crawled_time']}")
        if data.get('crawled_url'):
            md_lines.append(f"**ì›ë³¸ URL:** {data['crawled_url']}")
        md_lines.append("")
        
        md_lines.append("## ğŸ“‹ API ì •ë³´")
        md_lines.append("")
        md_lines.append("ì´ APIëŠ” LINK íƒ€ì…ìœ¼ë¡œ, ì™¸ë¶€ ë§í¬ë¥¼ í†µí•´ ì œê³µë©ë‹ˆë‹¤.")
        md_lines.append("")
        
        # í…Œì´ë¸” ì •ë³´
        table_info = data.get('info', {})
        if table_info:
            md_lines.append("## ğŸ“Š ìƒì„¸ ì •ë³´")
            md_lines.append("")
            for key, value in table_info.items():
                md_lines.append(f"**{key}:** {value}")
            md_lines.append("")
        
        # ê±´ë„ˆë›´ ì´ìœ 
        if data.get('skip_reason'):
            md_lines.append("## â„¹ï¸ ì²˜ë¦¬ ì •ë³´")
            md_lines.append("")
            md_lines.append(f"**ì²˜ë¦¬ ìƒíƒœ:** {data['skip_reason']}")
            md_lines.append("")
        
        # í‘¸í„°
        md_lines.append("## ğŸ“ ìƒì„± ì •ë³´")
        md_lines.append("")
        md_lines.append("ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        md_lines.append("**API íƒ€ì…:** LINK (ì™¸ë¶€ ë§í¬ ì œê³µ)")
        if data.get('api_id'):
            md_lines.append(f"**API ID:** {data['api_id']}")
        
        return "\n".join(md_lines)   
    
    @staticmethod
    def _swagger_to_markdown(data, md_lines):
        """Swagger APIë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        api_info = data.get('api_info', {})
        endpoints = data.get('endpoints', [])
        
        # ì œëª©
        title = api_info.get('title', 'API Documentation')
        md_lines.append(f"# {title}")
        md_lines.append("")
        
        # í¬ë¡¤ë§ ì •ë³´
        if data.get('crawled_time'):
            md_lines.append(f"**í¬ë¡¤ë§ ì‹œê°„:** {data['crawled_time']}")
        if data.get('crawled_url'):
            md_lines.append(f"**ì›ë³¸ URL:** {data['crawled_url']}")
        md_lines.append("")
        
        # API ê¸°ë³¸ ì •ë³´
        md_lines.append("## ğŸ“‹ API ì •ë³´")
        md_lines.append("")
        
        if api_info.get('description'):
            description = str(api_info['description']).replace('\n', ' ').strip()
            md_lines.append(f"**ì„¤ëª…:** {description}")
            md_lines.append("")
        
        # Base URL ì •ë³´
        if api_info.get('base_url'):
            md_lines.append(f"**Base URL:** `{api_info['base_url']}`")
            md_lines.append("")
        
        if api_info.get('schemes') and isinstance(api_info['schemes'], list):
            schemes_str = ", ".join(str(s) for s in api_info['schemes'])
            md_lines.append(f"**ì§€ì› í”„ë¡œí† ì½œ:** {schemes_str}")
            md_lines.append("")
        
        # ì—”ë“œí¬ì¸íŠ¸ ì •ë³´
        if endpoints and isinstance(endpoints, list):
            md_lines.append(f"## ğŸ”— API ì—”ë“œí¬ì¸íŠ¸ ({len(endpoints)}ê°œ)")
            md_lines.append("")
            
            # Base URLì´ ìˆìœ¼ë©´ ì™„ì „í•œ URL ì •ë³´ ì¶”ê°€
            base_url = api_info.get('base_url', '')
            if base_url:
                md_lines.append(f"**Base URL:** `{base_url}`")
                md_lines.append("")
            
            # ì„¹ì…˜ë³„ë¡œ ê·¸ë£¹í™”
            sections = {}
            for endpoint in endpoints:
                if not isinstance(endpoint, dict):
                    continue
                section = endpoint.get('section', 'Default')
                if section not in sections:
                    sections[section] = []
                sections[section].append(endpoint)
            
            for section_name, section_endpoints in sections.items():
                if len(sections) > 1:  # ì„¹ì…˜ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°ë§Œ ì„¹ì…˜ ì œëª© í‘œì‹œ
                    md_lines.append(f"### {section_name}")
                    md_lines.append("")
                
                for endpoint in section_endpoints:
                    try:
                        # ì—”ë“œí¬ì¸íŠ¸ ì œëª©
                        method = str(endpoint.get('method', 'GET')).upper()
                        path = str(endpoint.get('path', ''))
                        description = str(endpoint.get('description', '')).replace('\n', ' ').strip()
                        
                        # ì™„ì „í•œ URL ìƒì„± (Base URLì´ ìˆëŠ” ê²½ìš°)
                        full_url = f"{base_url}{path}" if base_url and path else path
                        
                        md_lines.append(f"#### `{method}` {path}")
                        if base_url:
                            md_lines.append(f"**ì™„ì „í•œ URL:** `{full_url}`")
                        md_lines.append("")
                        
                        if description:
                            md_lines.append(f"**ì„¤ëª…:** {description}")
                            md_lines.append("")
                        
                        # íŒŒë¼ë¯¸í„° ì •ë³´
                        parameters = endpoint.get('parameters', [])
                        if parameters and isinstance(parameters, list):
                            md_lines.append("**íŒŒë¼ë¯¸í„°:**")
                            md_lines.append("")
                            md_lines.append("| ì´ë¦„ | íƒ€ì… | í•„ìˆ˜ | ì„¤ëª… |")
                            md_lines.append("|------|------|------|------|")
                            
                            for param in parameters:
                                if not isinstance(param, dict):
                                    continue
                                name = str(param.get('name', '')).replace('|', '\\|')
                                param_type = str(param.get('type', '')).replace('|', '\\|')
                                required = "âœ…" if param.get('required', False) else "âŒ"
                                desc = str(param.get('description', '')).replace('\n', ' ').replace('|', '\\|')
                                
                                # ì„¤ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤„ì´ê¸°
                                if len(desc) > 50:
                                    desc = desc[:50] + "..."
                                
                                md_lines.append(f"| `{name}` | {param_type} | {required} | {desc} |")
                            
                            md_lines.append("")
                        
                        # ì‘ë‹µ ì •ë³´
                        responses = endpoint.get('responses', [])
                        if responses and isinstance(responses, list):
                            md_lines.append("**ì‘ë‹µ:**")
                            md_lines.append("")
                            md_lines.append("| ìƒíƒœ ì½”ë“œ | ì„¤ëª… |")
                            md_lines.append("|-----------|------|")
                            
                            for response in responses:
                                if not isinstance(response, dict):
                                    continue
                                status_code = str(response.get('status_code', '')).replace('|', '\\|')
                                desc = str(response.get('description', '')).replace('\n', ' ').replace('|', '\\|')
                                
                                # ì„¤ëª…ì´ ë„ˆë¬´ ê¸¸ë©´ ì¤„ì´ê¸°
                                if len(desc) > 80:
                                    desc = desc[:80] + "..."
                                
                                md_lines.append(f"| `{status_code}` | {desc} |")
                            
                            md_lines.append("")
                        
                        md_lines.append("---")
                        md_lines.append("")
                    except Exception as e:
                        print(f"âš ï¸ ì—”ë“œí¬ì¸íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
        
        # í‘¸í„°
        md_lines.append("## ğŸ“ ìƒì„± ì •ë³´")
        md_lines.append("")
        md_lines.append("ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        if data.get('api_id'):
            md_lines.append(f"**API ID:** {data['api_id']}")
        if api_info.get('base_url'):
            md_lines.append(f"**Base URL:** {api_info['base_url']}")
        
        return "\n".join(md_lines)
    
    @staticmethod
    def _general_api_to_markdown(data, md_lines):
        """ì¼ë°˜ APIë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        general_info = data.get('general_api_info', {})
        detail_info = general_info.get('detail_info', {})
        
        # ì œëª©
        title = detail_info.get('description', 'API Documentation')[:50] + "..." if len(detail_info.get('description', '')) > 50 else detail_info.get('description', 'API Documentation')
        md_lines.append(f"# {title}")
        md_lines.append("")
        
        # í¬ë¡¤ë§ ì •ë³´
        if data.get('crawled_time'):
            md_lines.append(f"**í¬ë¡¤ë§ ì‹œê°„:** {data['crawled_time']}")
        if data.get('crawled_url'):
            md_lines.append(f"**ì›ë³¸ URL:** {data['crawled_url']}")
        md_lines.append("")
        
        # ìƒì„¸ê¸°ëŠ¥ ì •ë³´
        if detail_info:
            md_lines.append("## ğŸ“‹ API ìƒì„¸ì •ë³´")
            md_lines.append("")
            
            if detail_info.get('description'):
                md_lines.append(f"**ê¸°ëŠ¥ ì„¤ëª…:**")
                md_lines.append(f"{detail_info['description']}")
                md_lines.append("")
            
            if detail_info.get('request_url'):
                md_lines.append(f"**ìš”ì²­ ì£¼ì†Œ:** `{detail_info['request_url']}`")
                md_lines.append("")
            
            if detail_info.get('service_url'):
                md_lines.append(f"**ì„œë¹„ìŠ¤ URL:** `{detail_info['service_url']}`")
                md_lines.append("")
            
            # í™œìš©ìŠ¹ì¸ ì ˆì°¨
            if detail_info.get('approval_process'):
                approval = detail_info['approval_process']
                md_lines.append("**í™œìš©ìŠ¹ì¸ ì ˆì°¨:**")
                if approval.get('development'):
                    md_lines.append(f"- ê°œë°œë‹¨ê³„: {approval['development']}")
                if approval.get('operation'):
                    md_lines.append(f"- ìš´ì˜ë‹¨ê³„: {approval['operation']}")
                md_lines.append("")
            
            # ì‹ ì²­ê°€ëŠ¥ íŠ¸ë˜í”½
            if detail_info.get('traffic_limit'):
                traffic = detail_info['traffic_limit']
                md_lines.append("**ì‹ ì²­ê°€ëŠ¥ íŠ¸ë˜í”½:**")
                if traffic.get('development'):
                    md_lines.append(f"- ê°œë°œê³„ì •: {traffic['development']}")
                if traffic.get('operation'):
                    md_lines.append(f"- ìš´ì˜ê³„ì •: {traffic['operation']}")
                md_lines.append("")
        
        # ìš”ì²­ë³€ìˆ˜
        request_params = general_info.get('request_parameters', [])
        if request_params:
            md_lines.append(f"## ğŸ“¤ ìš”ì²­ë³€ìˆ˜ ({len(request_params)}ê°œ)")
            md_lines.append("")
            md_lines.append("| í•­ëª©ëª…(êµ­ë¬¸) | í•­ëª©ëª…(ì˜ë¬¸) | í¬ê¸° | í•„ìˆ˜ì—¬ë¶€ | ìƒ˜í”Œë°ì´í„° | ì„¤ëª… |")
            md_lines.append("|--------------|--------------|------|----------|------------|------|")
            
            for param in request_params:
                name_kor = str(param.get('name_kor', '')).replace('|', '\\|')
                name_eng = str(param.get('name_eng', '')).replace('|', '\\|')
                size = str(param.get('size', '')).replace('|', '\\|')
                required = str(param.get('required', '')).replace('|', '\\|')
                sample = str(param.get('sample_data', '')).replace('|', '\\|')
                desc = str(param.get('description', '')).replace('|', '\\|')
                
                # ê¸´ í…ìŠ¤íŠ¸ ì¤„ì´ê¸°
                if len(sample) > 30:
                    sample = sample[:30] + "..."
                if len(desc) > 50:
                    desc = desc[:50] + "..."
                
                md_lines.append(f"| {name_kor} | `{name_eng}` | {size} | {required} | {sample} | {desc} |")
            
            md_lines.append("")
        
        # ì¶œë ¥ê²°ê³¼
        response_elements = general_info.get('response_elements', [])
        if response_elements:
            md_lines.append(f"## ğŸ“¥ ì¶œë ¥ê²°ê³¼ ({len(response_elements)}ê°œ)")
            md_lines.append("")
            md_lines.append("| í•­ëª©ëª…(êµ­ë¬¸) | í•­ëª©ëª…(ì˜ë¬¸) | í¬ê¸° | í•„ìˆ˜ì—¬ë¶€ | ìƒ˜í”Œë°ì´í„° | ì„¤ëª… |")
            md_lines.append("|--------------|--------------|------|----------|------------|------|")
            
            for element in response_elements:
                name_kor = str(element.get('name_kor', '')).replace('|', '\\|')
                name_eng = str(element.get('name_eng', '')).replace('|', '\\|')
                size = str(element.get('size', '')).replace('|', '\\|')
                required = str(element.get('required', '')).replace('|', '\\|')
                sample = str(element.get('sample_data', '')).replace('|', '\\|')
                desc = str(element.get('description', '')).replace('|', '\\|')
                
                # ê¸´ í…ìŠ¤íŠ¸ ì¤„ì´ê¸°
                if len(sample) > 30:
                    sample = sample[:30] + "..."
                if len(desc) > 50:
                    desc = desc[:50] + "..."
                
                md_lines.append(f"| {name_kor} | `{name_eng}` | {size} | {required} | {sample} | {desc} |")
            
            md_lines.append("")
        
        # í‘¸í„°
        md_lines.append("## ğŸ“ ìƒì„± ì •ë³´")
        md_lines.append("")
        md_lines.append("ì´ ë¬¸ì„œëŠ” ë‚˜ë¼ì¥í„° API í¬ë¡¤ëŸ¬ì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        md_lines.append("**API íƒ€ì…:** ì¼ë°˜ API (Swagger ë¯¸ì§€ì›)")
        if data.get('api_id'):
            md_lines.append(f"**API ID:** {data['api_id']}")
        
        return "\n".join(md_lines)
    
    @staticmethod
    def save_as_csv(data, file_path):
        """CSV í˜•íƒœë¡œ ì €ì¥ - ëª¨ë“  ë¬¸ì„œì˜ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ íŒŒì¼ì— ëˆ„ì """
        try:
            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            dir_path = os.path.dirname(file_path)
            if dir_path:
                os.makedirs(dir_path, exist_ok=True)
            
            # info ë°ì´í„° ì¶”ì¶œ
            info_data = data.get('info', {})
            if not info_data:
                return False, "ì €ì¥í•  í…Œì´ë¸” ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            # ì €ì¥í•  í•­ëª© ëª©ë¡
            target_fields = [
                'ë¶„ë¥˜ì²´ê³„',
                'ì œê³µê¸°ê´€',
                'ê´€ë¦¬ë¶€ì„œëª…',
                'ê´€ë¦¬ë¶€ì„œ ì „í™”ë²ˆí˜¸',
                'API ìœ í˜•',
                'ë°ì´í„°í¬ë§·',
                'í™œìš©ì‹ ì²­',
                'í‚¤ì›Œë“œ',
                'ë“±ë¡ì¼',
                'ìˆ˜ì •ì¼',
                'ë¹„ìš©ë¶€ê³¼ìœ ë¬´',
                'ì´ìš©í—ˆë½ë²”ìœ„'
            ]
            
            # ë¬¸ì„œ ë²ˆí˜¸ì™€ í¬ë¡¤ë§ ì‹œê°„ ì¶”ê°€
            filtered_data = {
                'ë¬¸ì„œë²ˆí˜¸': data.get('api_id', ''),
                'í¬ë¡¤ë§ì‹œê°„': data.get('crawled_time', ''),
                'URL': data.get('crawled_url', '')
            }
            
            # ì§€ì •ëœ í•­ëª©ë§Œ í•„í„°ë§í•˜ì—¬ ì¶”ê°€
            for field in target_fields:
                filtered_data[field] = info_data.get(field, '')
            
            # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            file_exists = os.path.isfile(file_path)
            
            # CSV íŒŒì¼ ì‘ì„± (cp949 ì¸ì½”ë”© ì‚¬ìš© - MS Office í˜¸í™˜)
            with open(file_path, 'a', encoding='cp949', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=filtered_data.keys())
                
                # íŒŒì¼ì´ ìƒˆë¡œ ìƒì„±ë˜ëŠ” ê²½ìš°ì—ë§Œ í—¤ë” ì‘ì„±
                if not file_exists:
                    writer.writeheader()
                
                # ë°ì´í„° ì‘ì„±
                writer.writerow(filtered_data)
            
            return True, None
        except Exception as e:
            return False, f"CSV ì €ì¥ ì‹¤íŒ¨: {str(e)}"

    @staticmethod
    def save_crawling_result(data, output_dir, api_id, formats=['json', 'xml']):
        """í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ - ê°œì„ ëœ ë¡œì§"""
        saved_files = []
        errors = []
        
        # í…Œì´ë¸” ì •ë³´ì—ì„œ ì œê³µê¸°ê´€ê³¼ ìˆ˜ì •ì¼ ì¶”ì¶œ
        table_info = data.get('info', {})
        org_name = table_info.get('ì œê³µê¸°ê´€', 'unknown_org')
        modified_date = table_info.get('ìˆ˜ì •ì¼', 'unknown_date')
        
        # URLì—ì„œ ë¬¸ì„œë²ˆí˜¸ ì¶”ì¶œ
        crawled_url = data.get('crawled_url', '')
        doc_num = 'unknown_doc'
        if crawled_url:
            match = re.search(r'/data/(\d+)/openapi\.do', crawled_url)
            if match:
                doc_num = match.group(1)
        
        # ê¸°ê´€ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        org_name = re.sub(r'[^\w\s-]', '', org_name)
        org_name = re.sub(r'[\s]+', '_', org_name).strip()
        
        # API ìœ í˜• í™•ì¸
        api_type = data.get('api_type', 'unknown')
        api_category = table_info.get('API ìœ í˜•', '')
        is_link_type = 'LINK' in api_category.upper() if api_category else False
        
        # API ìœ í˜•ì— ë”°ë¥¸ ìƒìœ„ ë””ë ‰í† ë¦¬ ì„¤ì • (ê°œì„ ëœ ë¡œì§)
        if api_type == 'link' or is_link_type:
            # LINK íƒ€ì…ì˜ ê²½ìš°
            base_output_dir = os.path.join(output_dir, 'LINK', org_name)
        elif api_type == 'general':
            # ì¼ë°˜ API (Swagger ë¯¸ì§€ì›)ì˜ ê²½ìš°
            base_output_dir = os.path.join(output_dir, 'ì¼ë°˜API_old', org_name)
        elif api_type == 'swagger':
            # Swagger APIì˜ ê²½ìš°
            base_output_dir = os.path.join(output_dir, 'ì¼ë°˜API', org_name)
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…
            base_output_dir = os.path.join(output_dir, 'ê¸°íƒ€', org_name)
        
        # íŒŒì¼ëª… ìƒì„±
        file_prefix = f"{doc_num}_{modified_date}"
        
        os.makedirs(base_output_dir, exist_ok=True)
        
        # ê° í˜•ì‹ë³„ë¡œ ì €ì¥
        for format_type in formats:
            try:
                if format_type == 'json':
                    file_path = os.path.join(base_output_dir, f"{file_prefix}.json")
                    success, error = DataExporter.save_as_json(data, file_path)
                    if success:
                        saved_files.append(file_path)
                elif format_type == 'xml':
                    file_path = os.path.join(base_output_dir, f"{file_prefix}.xml")
                    success, error = DataExporter.save_as_xml(data, file_path)
                    if success:
                        saved_files.append(file_path)
                elif format_type == 'md':
                    file_path = os.path.join(base_output_dir, f"{file_prefix}.md")
                    success, error = DataExporter.save_as_markdown(data, file_path)
                    if success:
                        saved_files.append(file_path)
                elif format_type == 'csv':
                    # CSV íŒŒì¼ì€ CSV ë””ë ‰í† ë¦¬ì— ì €ì¥ (ë‹¨ì¼ íŒŒì¼)
                    csv_dir = os.path.join(output_dir, 'CSV')
                    os.makedirs(csv_dir, exist_ok=True)
                    file_path = os.path.join(csv_dir, "all_table_info.csv")
                    success, error = DataExporter.save_as_csv(data, file_path)
                    if success:
                        saved_files.append(file_path)
            
            except Exception as e:
                error_msg = f"{format_type.upper()} ì €ì¥ ì‹¤íŒ¨: {str(e)}"
                print(f"âŒ {error_msg}")
                errors.append(error_msg)
        
        return saved_files, errors
    
    @staticmethod
    def save_table_info(data, output_dir, api_id):
        """í…Œì´ë¸” ì •ë³´ ì €ì¥"""
        try:
            # info ë””ë ‰í† ë¦¬ ìƒì„±
            info_dir = os.path.join(output_dir, 'info')
            os.makedirs(info_dir, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            file_name = f"{api_id}_table_info.json"
            file_path = os.path.join(info_dir, file_name)
            
            # JSONìœ¼ë¡œ ì €ì¥
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            return True, file_path
            
        except Exception as e:
            return False, f"í…Œì´ë¸” ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {str(e)}"